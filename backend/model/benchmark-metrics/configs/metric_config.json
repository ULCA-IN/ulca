[
    {
        "task_type":"translation", "metric_name":"bleu", "metric_implementation":"TranslationBLEUScoreEval", "score_context":"default"
    },
    {
        "task_type":"asr", "metric_name":"wer", "metric_implementation":"ASRWEREval", "score_context":"default"
    },
    {
        "task_type":"asr", "metric_name":"cer", "metric_implementation":"ASRCEREval", "score_context":"default"
    },
    {
        "task_type":"translation", "metric_name":"meteor", "metric_implementation":"TranslationMeteorScoreEval", "score_context":"default"
    },
    {
        "task_type":"translation", "metric_name":"rouge", "metric_implementation":"TranslationRougeScoreEval", "score_context":"default"
    },
    {
        "task_type":"translation", "metric_name":"bert", "metric_implementation":"TranslationBertScoreEval", "score_context":"default"
    },
    {
        "task_type":"translation", "metric_name":"gleu", "metric_implementation":"TranslationGleuScoreEval", "score_context":"default"
    },
    {
        "task_type":"translation", "metric_name":"ribes", "metric_implementation":"TranslationRibesScoreEval", "score_context":"default"
    },
    {
        "task_type":"ocr", "metric_name":"wer", "metric_implementation":"OCRWEREval", "score_context":"default"
    },
    {
        "task_type":"ocr", "metric_name":"cer", "metric_implementation":"OCRCEREval", "score_context":"default"
    },
    {
        "task_type":"transliteration", "metric_name":"cer", "metric_implementation":"TransliterationCEREval", "score_context":"default"
    },
    {
        "task_type":"transliteration", "metric_name":"top-1 accuracy", "metric_implementation":"TransliterationTopOneAccuracyEval", "score_context":"default"
    },
    {
        "task_type":"transliteration", "metric_name":"top-5 accuracy", "metric_implementation":"TransliterationTopFiveAccuracyEval", "score_context":"default"
    }
    
]